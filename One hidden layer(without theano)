
from __future__ import division
import numpy as np
from scipy import optimize
import sigmoid
from sigmoid import sigmoid as sig


class NN_1HL(object):
    
    def __init__(self, reg_lambda=0, epsilon_init=0.12, hidden_layer_size1=25,hidden_layer_size2=30, opti_method='TNC', maxiter=500):
        self.reg_lambda = reg_lambda
        self.epsilon_init = epsilon_init
        self.hidden_layer_size1 = hidden_layer_size1    #intialization of all 
        self.hidden_layer_size2= hidden_layer_size2
        self.activation_func = self.hyperbolic_tan
        self.activation_func_prime = self.hyperbolic_prime
        self.method = opti_method
        self.maxiter = maxiter
    
    def sigmoid(self, z):
       return 1 / (1 + np.exp(-z))
    
    def sigmoid_prime(self, z):
        sig = self.sigmoid(z)
        return sig * (1 - sig)

    def hyperbolic_tan(self,z):                                                                                     #activation function of neurons
        return np.tanh(z)

    def hyperbolic_prime(self,z):
        temp=self.hyperbolic_tan(z)
        return 1-(temp*temp)
    
    def sumsqr(self, a):
        return np.sum(a ** 2)
    
    def rand_init(self, l_in, l_out):
        return np.random.rand(l_out, l_in + 1) * 2 * self.epsilon_init - self.epsilon_init
    
    def pack_thetas(self, t1, t2):
        return np.concatenate((t1.reshape(-1), t2.reshape(-1)))
    
    def unpack_thetas(self, thetas, input_layer_size, hidden_layer_size, num_labels):
        t1_start = 0
        t1_end = hidden_layer_size * (input_layer_size + 1)
        t1 = thetas[t1_start:t1_end].reshape((hidden_layer_size, input_layer_size + 1))
        t2 = thetas[t1_end:].reshape((num_labels, hidden_layer_size + 1))
        return t1, t2
    
    def _forward(self, X, t1, t2,t3):
        m = X.shape[0]
        ones = None
        if len(X.shape) == 1:
            ones = np.array(1).reshape(1,)
        else:
            ones = np.ones(m).reshape(m,1)
        
        # Input layer
        a1 = np.hstack((ones, X))
        
        # Hidden Layer
        z2 = np.dot(t1, a1.T)                                                                        #60000*785,530*785,785*60000==530*60000
        a2 = self.activation_func(z2)
        
        a2 = np.hstack((ones, a2.T))
        
        # Output layer
        z3 = np.dot(t2, a2.T)
        a3 = self.activation_func(z3)
        a3 = np.hstack((ones,a3.T))
        z4 = np.dot(t3,a3.T)
        a4 = self.activation_func(z4)
        return a1, z2, a2, z3, a3 , z4,a4
    
    def function(self, t1,t2, t3, input_layer_size, hidden_layer_size, num_labels, X, y, reg_lambda):
        
        
        m = X.shape[0]
        #Y = np.eye(num_labels)[y]
        Y=np.zeros(num_labels*len(y))
        Y=Y.reshape(num_labels,len(y))
        print(num_labels,len(y))
        for i in range(0,len(y)):
            temp=y[i]                                                                                                     #calculation of the cost function
            Y[temp,i]=1
        _, _, _, _, h = self._forward(X, t1, t2)
        costPositive = -Y * np.log(h)
        costNegative = (1 - Y) * np.log(1 - h)
        cost = costPositive - costNegative
        J = np.sum(cost) / m
        
        if reg_lambda != 0:
            t1f = t1[:, 1:]
            t2f = t2[:, 1:]
            t3f = t3[:, 1:]
            reg = (self.reg_lambda / (2 * m)) * (self.sumsqr(t1f) + self.sumsqr(t2f)+self.sumsqr(t3f))
            J = J + reg
        return J
        
    def function_prime(self, t1,t2,t3, input_layer_size, hidden_layer_size, num_labels, X, y, reg_lambda):
        
        
        m = X.shape[0]
        t1f = t1[:, 1:]
        t2f = t2[:, 1:]
        t3f = t3[:, 1:]
        #Y = np.eye(num_labels)[y]

        Y=np.zeros(num_labels*len(y))
        Y=Y.reshape(num_labels,len(y))
        print(y)
        for i in range(0,len(y)):
            temp=y[i]
            Y[temp,i]=1
        
        Delta1, Delta2 = 0, 0
        for i, row in enumerate(X):
            a1, z2, a2, z3, a3,z4,a4 = self._forward(row, t1, t2 ,t3)                                                     #Backpropagation and calculation of gradient as sum(error*activation function)
            
            # Backprop
            d4 = a4 - Y[:, i]
            d3 = np.dot(d4,t3f) * self.activation_func_prime(z3)
            d2 = np.dot(d3,t2f) * self.activation_func_prime(z2)
            
            Delta3 += np.dot(d4[np.newaxis].T, a3[np.newaxis])
            Delta2 += np.dot(d3[np.newaxis].T, a2[np.newaxis])
            Delta1 += np.dot(d2[np.newaxis].T, a1[np.newaxis])
            
        Theta1_grad = (1 / m) * Delta1
        Theta2_grad = (1 / m) * Delta2
        Theta3_grad = (1 / m) * Delta3
        
        if reg_lambda != 0:
            Theta1_grad[:, 1:] = Theta1_grad[:, 1:] + (reg_lambda / m) * t1f
            Theta2_grad[:, 1:] = Theta2_grad[:, 1:] + (reg_lambda / m) * t2f
            Theta3_grad[:, 1:] = Theta3_grad[:, 1:] + (reg_lambda / m) * t3f
        
        return (Theta1_grad, Theta2_grad, Theta3_grad)
    
    def optimize_func (self,input_vector,correct_output,hidden_layer_size1,hidden_layer_size2,No_classes,maxiter,epsilon,lambda_init,learning_rate):
        theta1=self.rand_init((len(input_vector[1])),hidden_layer_size)
        theta2=self.rand_init(hidden_layer_size1,hidden_layer_size2)
        theta3=self.rand_init(hidden_layer_size2,No_Classes)
        iteration=0
        while(iteration<10):
            a1, z2, a2, z3, a3,z4,a4=self._forward(input_vector,theta1,theta2)
            J=self.function(theta1,theta2,theta3,len(input_vector[1]),hidden_layer_size1,hidden_layer_size3,No_classes,input_vector,correct_output,lambda_init)
            theta1_grad,theta2_grad=self.function_prime(theta1,theta2,(len(input_vector[1])),hidden_layer_size,No_classes,input_vector,correct_output,lambda_init)
            theta1 =theta1 - learning_rate*theta1_grad
            theta2=theta2 - learning_rate*theta2_grad
            iteration += 1
        return theta1,theta2   

    def predict(self, X):
        return self.predict_proba(X).argmax(0)
    
    def predict_proba(self, X):
        _, _, _, _,_,_, h = self._forward(X, self.t1, self.t2)
        return h
